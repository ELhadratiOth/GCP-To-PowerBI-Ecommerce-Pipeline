{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ee37f4-d678-4a6c-a051-ccad7e1d9c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Star Schema Transformation\n",
    "**Transform Staging Tables to Star Schema for Analytics**\n",
    "\n",
    "## Pipeline Overview\n",
    "This notebook transforms staging tables from Delta Lake into a dimensional star schema:\n",
    "- **Input**: 9 staging tables in `staging` database (with CDC metadata)\n",
    "- **Process**: Filter deleted records, deduplicate by ID, create fact and dimension tables\n",
    "- **Output**: 5 star schema tables ready for Snowflake export\n",
    "\n",
    "**Star Schema Structure:**\n",
    "- `FACT_ORDERS` - Central fact table with all measures\n",
    "- `DIM_CUSTOMERS` - Customer dimension (enriched with geolocation)\n",
    "- `DIM_SELLERS` - Seller dimension (enriched with geolocation)\n",
    "- `DIM_PRODUCTS` - Product dimension (with Spanish & English categories)\n",
    "- `DIM_DATE` - Date dimension (generated from order dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181e60cb-d7df-4700-b58e-c0d74e6efe91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\nSTAR SCHEMA TRANSFORMATION PIPELINE\n====================================================================================================\nSpark Version: 3.5.2\nTimestamp: 2025-11-09 18:03:39\n====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime , timedelta\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"star-schema-transformation\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"STAR SCHEMA TRANSFORMATION PIPELINE\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d305acc-4199-421d-8f1d-230574bb364e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SECTION : Load Staging Tables from Delta Lake\n",
    "Load all staging tables with CDC metadata (_fivetran_synced, _fivetran_deleted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e5776b-af19-4159-8cab-31b53f9d88cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nLoading staging tables from Delta Lake...\n\nStaging Tables Loaded (with CDC metadata):\n\n✓ stg_customers:                              99,441 rows\n✓ stg_orders:                                 99,441 rows\n✓ stg_order_items:                           112,650 rows\n✓ stg_payments:                              103,886 rows\n✓ stg_reviews:                                99,224 rows\n✓ stg_products:                               32,951 rows\n✓ stg_sellers:                                 3,095 rows\n✓ stg_product_category_translation:               71 rows\n✓ stg_geolocation:                           738,332 rows\n✓ CDC deduplication will be applied in next section\n"
     ]
    }
   ],
   "source": [
    "# Load all staging tables (with CDC metadata)\n",
    "print(\"\\nLoading staging tables from Delta Lake...\\n\")\n",
    "\n",
    "stg_customers = spark.read.table(\"staging.stg_customers\")\n",
    "stg_orders = spark.read.table(\"staging.stg_orders\")\n",
    "stg_order_items = spark.read.table(\"staging.stg_order_items\")\n",
    "stg_payments = spark.read.table(\"staging.stg_payments\")\n",
    "stg_reviews = spark.read.table(\"staging.stg_reviews\")\n",
    "stg_products = spark.read.table(\"staging.stg_products\")\n",
    "stg_sellers = spark.read.table(\"staging.stg_sellers\")\n",
    "stg_product_category_translation = spark.read.table(\"staging.stg_product_category_translation\")\n",
    "stg_geolocation = spark.read.table(\"staging.stg_geolocation\")\n",
    "\n",
    "# Display row counts (includes deleted and duplicates)\n",
    "print(\"Staging Tables Loaded (with CDC metadata):\\n\")\n",
    "print(f\"✓ stg_customers:                        {stg_customers.count():>12,} rows\")\n",
    "print(f\"✓ stg_orders:                           {stg_orders.count():>12,} rows\")\n",
    "print(f\"✓ stg_order_items:                      {stg_order_items.count():>12,} rows\")\n",
    "print(f\"✓ stg_payments:                         {stg_payments.count():>12,} rows\")\n",
    "print(f\"✓ stg_reviews:                          {stg_reviews.count():>12,} rows\")\n",
    "print(f\"✓ stg_products:                         {stg_products.count():>12,} rows\")\n",
    "print(f\"✓ stg_sellers:                          {stg_sellers.count():>12,} rows\")\n",
    "print(f\"✓ stg_product_category_translation:     {stg_product_category_translation.count():>12,} rows\")\n",
    "print(f\"✓ stg_geolocation:                      {stg_geolocation.count():>12,} rows\")\n",
    "\n",
    "print(\"✓ CDC deduplication will be applied in next section\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a66007-5f9e-41de-bf9f-0a1dc68a8d0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SECTION : CDC Deduplication & Deletion Filtering\n",
    "Filter deleted records and deduplicate by ID for tables with unique identifiers.\n",
    "\n",
    "**Deduplication Rules:**\n",
    "- Tables WITH unique ID: Filter deleted → Deduplicate by ID (keep latest by _fivetran_synced)\n",
    "- Tables WITHOUT unique ID or normal duplicates: Filter deleted only (NO deduplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b4e358-50a5-4ae2-9853-260f03f1f86c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTables with unique ID (applying deduplication):\n────────────────────────────────────────────────────────────────────────────────────────────────────\n\nCUSTOMERS:\n  Initial rows: 99,441\n  Deleted rows filtered: 0\n  Duplicates removed (by customer_id): 0\n  Final rows: 99,441\n\nORDERS:\n  Initial rows: 99,441\n  Deleted rows filtered: 0\n  Duplicates removed (by order_id): 0\n  Final rows: 99,441\n\nPRODUCTS:\n  Initial rows: 32,951\n  Deleted rows filtered: 0\n  Duplicates removed (by product_id): 0\n  Final rows: 32,951\n\nSELLERS:\n  Initial rows: 3,095\n  Deleted rows filtered: 0\n  Duplicates removed (by seller_id): 0\n  Final rows: 3,095\n\nREVIEWS:\n  Initial rows: 99,224\n  Deleted rows filtered: 0\n  Duplicates removed (by review_id): 814\n  Final rows: 98,410\n\nGEOLOCATION :\n  Initial rows: 738,332\n  Deleted rows filtered: 0\n  Duplicates removed (by geolocation_zip_code_prefix): 719,317\n  Final rows: 19,015\n\n\nTables without unique ID or with normal duplicates (NO deduplication):\n────────────────────────────────────────────────────────────────────────────────────────────────────\n\nORDER_ITEMS (duplicates are normal):\n  Initial rows: 112,650\n  Deleted rows filtered: 0\n  ✓ NO deduplication (duplicates are normal or no unique ID)\n  Final rows: 112,650\n\nPAYMENTS (multiple payments per order):\n  Initial rows: 103,886\n  Deleted rows filtered: 0\n  ✓ NO deduplication (duplicates are normal or no unique ID)\n  Final rows: 103,886\n\nPRODUCT_CATEGORY_TRANSLATION (no ID column):\n  Initial rows: 71\n  Deleted rows filtered: 0\n  ✓ NO deduplication (duplicates are normal or no unique ID)\n  Final rows: 71\n\n✓ CDC deduplication and deletion filtering completed\n"
     ]
    }
   ],
   "source": [
    "def deduplicate_by_id(df, id_column, table_name):\n",
    "    \"\"\"\n",
    "    Filter deleted records and deduplicate by ID column\n",
    "    \n",
    "    Steps:\n",
    "    1. Filter _fivetran_deleted = FALSE\n",
    "    2. Deduplicate by id_column, keep row with MAX(_fivetran_synced)\n",
    "    3. Drop CDC metadata columns\n",
    "    \"\"\"\n",
    "    print(f\"\\n{table_name}:\")\n",
    "    initial_count = df.count()\n",
    "    \n",
    "    # Step 1: Filter deleted records\n",
    "    df_active = df.filter(F.col(\"_fivetran_deleted\") == False)\n",
    "    deleted_count = initial_count - df_active.count()\n",
    "    print(f\"  Initial rows: {initial_count:,}\")\n",
    "    print(f\"  Deleted rows filtered: {deleted_count:,}\")\n",
    "    \n",
    "    # Step 2: Deduplicate by ID (keep latest by _fivetran_synced)\n",
    "    window_spec = Window.partitionBy(id_column).orderBy(F.col(\"_fivetran_synced\").desc())\n",
    "    df_dedup = df_active.withColumn(\"_rn\", F.row_number().over(window_spec)) \\\n",
    "        .filter(F.col(\"_rn\") == 1) \\\n",
    "        .drop(\"_rn\")\n",
    "    \n",
    "    duplicates_removed = df_active.count() - df_dedup.count()\n",
    "    print(f\"  Duplicates removed (by {id_column}): {duplicates_removed:,}\")\n",
    "    \n",
    "    # Step 3: Drop CDC metadata columns\n",
    "    df_clean = df_dedup.drop(\"_fivetran_synced\", \"_fivetran_deleted\", \"_fivetran_id\")\n",
    "    \n",
    "    final_count = df_clean.count()\n",
    "    print(f\"  Final rows: {final_count:,}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def filter_deleted_only(df, table_name):\n",
    "    \"\"\"\n",
    "    Filter deleted records ONLY (NO deduplication)\n",
    "    \n",
    "    For tables without unique ID or where duplicates are normal\n",
    "    \"\"\"\n",
    "    print(f\"\\n{table_name}:\")\n",
    "    initial_count = df.count()\n",
    "    \n",
    "    # Filter deleted records only\n",
    "    df_active = df.filter(F.col(\"_fivetran_deleted\") == False)\n",
    "    deleted_count = initial_count - df_active.count()\n",
    "    print(f\"  Initial rows: {initial_count:,}\")\n",
    "    print(f\"  Deleted rows filtered: {deleted_count:,}\")\n",
    "    print(f\"  ✓ NO deduplication (duplicates are normal or no unique ID)\")\n",
    "    \n",
    "    # Drop CDC metadata columns\n",
    "    df_clean = df_active.drop(\"_fivetran_synced\", \"_fivetran_deleted\", \"_fivetran_id\")\n",
    "    \n",
    "    final_count = df_clean.count()\n",
    "    print(f\"  Final rows: {final_count:,}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# ============================================================================\n",
    "# TABLES WITH UNIQUE ID - Deduplicate by ID\n",
    "# ============================================================================\n",
    "print(\"\\nTables with unique ID (applying deduplication):\")\n",
    "print(\"─\" * 100)\n",
    "\n",
    "customers_dedup = deduplicate_by_id(stg_customers, \"customer_id\", \"CUSTOMERS\")\n",
    "orders_dedup = deduplicate_by_id(stg_orders, \"order_id\", \"ORDERS\")\n",
    "products_dedup = deduplicate_by_id(stg_products, \"product_id\", \"PRODUCTS\")\n",
    "sellers_dedup = deduplicate_by_id(stg_sellers, \"seller_id\", \"SELLERS\")\n",
    "reviews_dedup = deduplicate_by_id(stg_reviews, \"review_id\", \"REVIEWS\")\n",
    "geolocation_dedup = deduplicate_by_id(stg_geolocation,\"geolocation_zip_code_prefix\",\"GEOLOCATION \")\n",
    "\n",
    "# ============================================================================\n",
    "# TABLES WITHOUT UNIQUE ID OR NORMAL DUPLICATES - Filter deleted only\n",
    "# ============================================================================\n",
    "print(\"\\n\\nTables without unique ID or with normal duplicates (NO deduplication):\")\n",
    "print(\"─\" * 100)\n",
    "\n",
    "order_items_active = filter_deleted_only(stg_order_items, \"ORDER_ITEMS (duplicates are normal)\")\n",
    "payments_active = filter_deleted_only(stg_payments, \"PAYMENTS (multiple payments per order)\")\n",
    "translation_active = filter_deleted_only(stg_product_category_translation, \"PRODUCT_CATEGORY_TRANSLATION (no ID column)\")\n",
    "\n",
    "print(\"\\n✓ CDC deduplication and deletion filtering completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "993d36f5-4a9d-4ad3-ae8d-7fde8d217d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SECTION : Create Orders Enrichment\n",
    "Calculate delivery metrics from deduplicated orders table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee258aa9-110a-4e5f-a8f3-222f49a20b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nCalculating delivery metrics...\n  Total orders: 99,441\n  On-time: 88,649 | Delayed: 7,827 | Pending: 2,965\n\n✓ Orders enrichment completed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCalculating delivery metrics...\")\n",
    "\n",
    "# Add delivery_days and delivery_delay_flag to orders\n",
    "orders_enriched = orders_dedup.withColumn(\n",
    "    \"delivery_days\",\n",
    "    F.when(\n",
    "        F.col(\"order_delivered_customer_date\").isNull(),\n",
    "        -1\n",
    "    ).otherwise(\n",
    "        F.datediff(\n",
    "            F.col(\"order_delivered_customer_date\").cast(\"date\"),\n",
    "            F.col(\"order_purchase_timestamp\").cast(\"date\")\n",
    "        )\n",
    "    )\n",
    ").withColumn(\n",
    "    \"delivery_delay_flag\",\n",
    "    F.when(\n",
    "        F.col(\"order_delivered_customer_date\").isNull(),\n",
    "        \"pending\"\n",
    "    ).when(\n",
    "        F.col(\"order_delivered_customer_date\") > F.col(\"order_estimated_delivery_date\"),\n",
    "        \"delayed\"\n",
    "    ).otherwise(\"on_time\")\n",
    ")\n",
    "\n",
    "# Statistics\n",
    "delayed_count = orders_enriched.filter(F.col(\"delivery_delay_flag\") == \"delayed\").count()\n",
    "pending_count = orders_enriched.filter(F.col(\"delivery_delay_flag\") == \"pending\").count()\n",
    "on_time_count = orders_enriched.filter(F.col(\"delivery_delay_flag\") == \"on_time\").count()\n",
    "\n",
    "print(f\"  Total orders: {orders_enriched.count():,}\")\n",
    "print(f\"  On-time: {on_time_count:,} | Delayed: {delayed_count:,} | Pending: {pending_count:,}\")\n",
    "\n",
    "print(\"\\n✓ Orders enrichment completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e336ec-475c-408e-a7d0-88a304aba8a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SECTION : Aggregate Payments and Reviews\n",
    "Aggregate payments and reviews by order_id from deduplicated tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a918578-2127-4c2d-8cd1-60648eb8eb82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n====================================================================================================\nSECTION 4: AGGREGATE PAYMENTS AND REVIEWS\n====================================================================================================\n\nAggregating payments by order_id...\n  ✓ Payments aggregated: 99,440 orders\n\nAggregating reviews by order_id...\n  ✓ Reviews aggregated: 98,129 orders\n\n✓ Aggregations completed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"SECTION 4: AGGREGATE PAYMENTS AND REVIEWS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Aggregate payments by order_id\n",
    "print(\"\\nAggregating payments by order_id...\")\n",
    "\n",
    "payments_agg = payments_active.groupBy(\"order_id\").agg(\n",
    "    F.sum(\"payment_value\").alias(\"total_payment_value\"),\n",
    "    F.count(\"*\").alias(\"payment_count\"),\n",
    "    F.max(\"payment_installments\").alias(\"max_installments\")\n",
    ")\n",
    "\n",
    "print(f\"  ✓ Payments aggregated: {payments_agg.count():,} orders\")\n",
    "\n",
    "# Aggregate reviews by order_id\n",
    "print(\"\\nAggregating reviews by order_id...\")\n",
    "\n",
    "reviews_agg = reviews_dedup.groupBy(\"order_id\").agg(\n",
    "    F.avg(\"review_score\").alias(\"avg_review_score\"),\n",
    "    F.count(\"*\").alias(\"total_reviews\")\n",
    ")\n",
    "\n",
    "print(f\"  ✓ Reviews aggregated: {reviews_agg.count():,} orders\")\n",
    "\n",
    "print(\"\\n✓ Aggregations completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7402c0b6-3679-44f0-a5ee-ee44130c1ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SECTION : Enrich Products with English Category Names\n",
    "Join deduplicated products with translation table to add English category names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee3aff6-8214-4c14-a21b-11a6b488b3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nJoining products with translation table...\n  ✓ Products enriched: 32,951\n  ✓ All products have English translations\n\n✓ Product enrichment completed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nJoining products with translation table...\")\n",
    "\n",
    "# Join products with translation (both already filtered and deduplicated)\n",
    "products_enriched = products_dedup.join(\n",
    "    translation_active.drop(\"_fivetran_synced\", \"_fivetran_deleted\", \"_fivetran_id\"),\n",
    "    on=\"product_category_name\",\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"product_category_name_english\",\n",
    "    F.when(\n",
    "        F.col(\"product_category_name_english\").isNull(),\n",
    "        F.col(\"product_category_name\")\n",
    "    ).otherwise(F.col(\"product_category_name_english\"))\n",
    ")\n",
    "\n",
    "print(f\"  ✓ Products enriched: {products_enriched.count():,}\")\n",
    "\n",
    "# Check for missing translations\n",
    "null_english = products_enriched.filter(F.col(\"product_category_name_english\").isNull()).count()\n",
    "if null_english > 0:\n",
    "    print(f\"  ⚠ Warning: {null_english:,} products have no English translation\")\n",
    "else:\n",
    "    print(f\"  ✓ All products have English translations\")\n",
    "\n",
    "print(\"\\n✓ Product enrichment completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a5ab92-6c93-4e48-aaf5-d216312309b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SECTION : Create DIM_CUSTOMERS (Enriched with Geolocation)\n",
    "Embed geolocation data (city, state, lat, lng) into customer dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb920916-f2bf-405d-955e-c61f6c860d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nJoining customers with geolocation...\n  ✓ DIM_CUSTOMERS created: 99,441 rows\n  ✓ All customers have geolocation data\n\nDIM_CUSTOMERS columns: ['customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state', 'geolocation_lat', 'geolocation_lng']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nJoining customers with geolocation...\")\n",
    "\n",
    "dim_customers = customers_dedup.join(\n",
    "    geolocation_dedup,\n",
    "    customers_dedup.customer_zip_code_prefix == geolocation_dedup.geolocation_zip_code_prefix,\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    customers_dedup.customer_id,\n",
    "    customers_dedup.customer_unique_id,\n",
    "    customers_dedup.customer_zip_code_prefix,\n",
    "    geolocation_dedup.geolocation_city.alias(\"customer_city\"),\n",
    "    geolocation_dedup.geolocation_state.alias(\"customer_state\"),\n",
    "    geolocation_dedup.geolocation_lat,\n",
    "    geolocation_dedup.geolocation_lng\n",
    ")\n",
    "\n",
    "print(f\"  ✓ DIM_CUSTOMERS created: {dim_customers.count():,} rows\")\n",
    "\n",
    "# Check for missing geolocation\n",
    "null_geo = dim_customers.filter(F.col(\"customer_city\").isNull()).count()\n",
    "if null_geo > 0:\n",
    "    print(f\"  ⚠ Warning: {null_geo:,} customers have no geolocation data\")\n",
    "else:\n",
    "    print(f\"  ✓ All customers have geolocation data\")\n",
    "\n",
    "print(\"\\nDIM_CUSTOMERS columns:\", dim_customers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec901a8-c4a7-4496-8291-df22a63750e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SECTION : Create DIM_SELLERS (Enriched with Geolocation)\n",
    "Embed geolocation data (city, state, lat, lng) into seller dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4141277-190a-46ca-9892-087093f22096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nJoining sellers with geolocation...\n  ✓ DIM_SELLERS created: 3,095 rows\n  ✓ All sellers have geolocation data\n\nDIM_SELLERS columns: ['seller_id', 'seller_zip_code_prefix', 'seller_city', 'seller_state', 'geolocation_lat', 'geolocation_lng']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nJoining sellers with geolocation...\")\n",
    "\n",
    "dim_sellers = sellers_dedup.join(\n",
    "    geolocation_dedup,\n",
    "    sellers_dedup.seller_zip_code_prefix == geolocation_dedup.geolocation_zip_code_prefix,\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    sellers_dedup.seller_id,\n",
    "    sellers_dedup.seller_zip_code_prefix,\n",
    "    geolocation_dedup.geolocation_city.alias(\"seller_city\"),\n",
    "    geolocation_dedup.geolocation_state.alias(\"seller_state\"),\n",
    "    geolocation_dedup.geolocation_lat,\n",
    "    geolocation_dedup.geolocation_lng\n",
    ")\n",
    "\n",
    "print(f\"  ✓ DIM_SELLERS created: {dim_sellers.count():,} rows\")\n",
    "\n",
    "# Check for missing geolocation\n",
    "null_geo = dim_sellers.filter(F.col(\"seller_city\").isNull()).count()\n",
    "if null_geo > 0:\n",
    "    print(f\"  ⚠ Warning: {null_geo:,} sellers have no geolocation data\")\n",
    "else:\n",
    "    print(f\"  ✓ All sellers have geolocation data\")\n",
    "\n",
    "print(\"\\nDIM_SELLERS columns:\", dim_sellers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb8ec044-893f-4b25-bf82-e8c8f2943d28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SECTION : Create DIM_PRODUCTS (with Spanish & English Categories)\n",
    "Create product dimension with both Spanish and English category names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a93af1d0-768a-424e-b140-47c908bacc9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nCreating DIM_PRODUCTS with both category names...\n  ✓ DIM_PRODUCTS created: 32,951 rows\n\nDIM_PRODUCTS columns: ['product_id', 'product_category_name', 'product_category_name_english', 'product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']\n\nSample:\n+--------------------------------+---------------------+-----------------------------+\n|product_id                      |product_category_name|product_category_name_english|\n+--------------------------------+---------------------+-----------------------------+\n|00066f42aeeb9f3007548bb9d3f33c38|perfumaria           |perfumery                    |\n|00088930e925c41fd95ebfe695fd2655|automotivo           |auto                         |\n|0009406fd7479715e4bef61dd91f2462|cama_mesa_banho      |bed_bath_table               |\n|000b8f95fcb9e0096488278317764d19|utilidades_domesticas|housewares                   |\n|000d9be29b5207b54e86aa1b1ac54872|relogios_presentes   |watches_gifts                |\n+--------------------------------+---------------------+-----------------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreating DIM_PRODUCTS with both category names...\")\n",
    "\n",
    "dim_products = products_enriched.select(\n",
    "    \"product_id\",\n",
    "    \"product_category_name\",\n",
    "    \"product_category_name_english\",\n",
    "    \"product_name_lenght\",\n",
    "    \"product_description_lenght\",\n",
    "    \"product_photos_qty\",\n",
    "    \"product_weight_g\",\n",
    "    \"product_length_cm\",\n",
    "    \"product_height_cm\",\n",
    "    \"product_width_cm\"\n",
    ")\n",
    "\n",
    "print(f\"  ✓ DIM_PRODUCTS created: {dim_products.count():,} rows\")\n",
    "\n",
    "print(\"\\nDIM_PRODUCTS columns:\", dim_products.columns)\n",
    "print(\"\\nSample:\")\n",
    "dim_products.select(\"product_id\", \"product_category_name\", \"product_category_name_english\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba939d97-48b2-437d-8170-27f8f50b79ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SECTION : Create DIM_DATE\n",
    "Generate date dimension from minimum order date to maximum estimated delivery date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb2abed7-b166-43e5-ae2b-11ab17482c44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nCalculating date range...\n  Min date: 2016-09-04 21:15:19\n  Max date: 2018-11-12 00:00:00\n  Date range: 798 days\n  ✓ Generated 799 dates\n  ✓ DIM_DATE created: 799 rows\n\nSample:\n+-------------------+--------+----+-----+---+-------+------------+-----------+---------+----------+----------+\n|date               |date_id |year|month|day|quarter|week_of_year|day_of_week|day_name |month_name|is_weekend|\n+-------------------+--------+----+-----+---+-------+------------+-----------+---------+----------+----------+\n|2016-09-04 21:15:19|20160904|2016|9    |4  |3      |35          |1          |Sunday   |September |1         |\n|2016-09-05 21:15:19|20160905|2016|9    |5  |3      |36          |2          |Monday   |September |0         |\n|2016-09-06 21:15:19|20160906|2016|9    |6  |3      |36          |3          |Tuesday  |September |0         |\n|2016-09-07 21:15:19|20160907|2016|9    |7  |3      |36          |4          |Wednesday|September |0         |\n|2016-09-08 21:15:19|20160908|2016|9    |8  |3      |36          |5          |Thursday |September |0         |\n+-------------------+--------+----+-----+---+-------+------------+-----------+---------+----------+----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Get date range from orders\n",
    "print(\"\\nCalculating date range...\")\n",
    "\n",
    "date_range = orders_enriched.agg(\n",
    "    F.min(\"order_purchase_timestamp\").alias(\"min_date\"),\n",
    "    F.max(\"order_estimated_delivery_date\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "min_date = date_range[\"min_date\"]\n",
    "max_date = date_range[\"max_date\"]\n",
    "\n",
    "print(f\"  Min date: {min_date}\")\n",
    "print(f\"  Max date: {max_date}\")\n",
    "\n",
    "# Generate date list\n",
    "date_diff = (max_date - min_date).days\n",
    "print(f\"  Date range: {date_diff:,} days\")\n",
    "\n",
    "date_list = []\n",
    "current_date = min_date\n",
    "while current_date <= max_date:\n",
    "    date_list.append((current_date,))\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "print(f\"  ✓ Generated {len(date_list):,} dates\")\n",
    "\n",
    "# Create date dimension\n",
    "dim_date = spark.createDataFrame(date_list, [\"date\"])\n",
    "\n",
    "dim_date = dim_date.withColumn(\n",
    "    \"date_id\", \n",
    "    F.date_format(F.col(\"date\"), \"yyyyMMdd\").cast(\"int\")\n",
    ").withColumn(\n",
    "    \"year\", \n",
    "    F.year(F.col(\"date\"))\n",
    ").withColumn(\n",
    "    \"month\", \n",
    "    F.month(F.col(\"date\"))\n",
    ").withColumn(\n",
    "    \"day\", \n",
    "    F.dayofmonth(F.col(\"date\"))\n",
    ").withColumn(\n",
    "    \"quarter\", \n",
    "    F.quarter(F.col(\"date\"))\n",
    ").withColumn(\n",
    "    \"week_of_year\", \n",
    "    F.weekofyear(F.col(\"date\"))\n",
    ").withColumn(\n",
    "    \"day_of_week\", \n",
    "    F.dayofweek(F.col(\"date\"))\n",
    ").withColumn(\n",
    "    \"day_name\", \n",
    "    F.date_format(F.col(\"date\"), \"EEEE\")\n",
    ").withColumn(\n",
    "    \"month_name\", \n",
    "    F.date_format(F.col(\"date\"), \"MMMM\")\n",
    ").withColumn(\n",
    "    \"is_weekend\",\n",
    "    F.when(F.col(\"day_of_week\").isin([1, 7]), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(f\"  ✓ DIM_DATE created: {dim_date.count():,} rows\")\n",
    "\n",
    "print(\"\\nSample:\")\n",
    "dim_date.orderBy(\"date\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81035d2-fa91-436c-b2ff-ef863239f42f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SECTION : Create FACT_ORDERS\n",
    "Build the central fact table with all measures and dimension foreign keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c20b82ce-42ee-403e-ad68-0eab2438d8df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nBuilding fact table from order items...\n  ✓ Base: 112,650 rows\n\nJoining with orders...\n  ✓ Orders joined: 112,650 rows\n\nJoining with payments...\n  ✓ Payments joined: 112,650 rows\n\nJoining with reviews...\n  ✓ Reviews joined: 112,650 rows\n\nCreating date dimension keys...\n\nCreating calculated measures...\n\n✓ FACT_ORDERS created: 112,650 rows\n\nFACT_ORDERS Statistics:\n-RECORD 0---------------------------\n total_rows         | 112650        \n unique_orders      | 98666         \n unique_customers   | 98666         \n unique_products    | 32951         \n unique_sellers     | 3095          \n delivered_orders   | 110196        \n on_time_deliveries | 101481        \n total_revenue      | 2.030813471E7 \n total_profit       | 4356670.37    \n\n"
     ]
    }
   ],
   "source": [
    "# Start with order items (NO deduplication applied)\n",
    "print(\"\\nBuilding fact table from order items...\")\n",
    "\n",
    "fact_orders = order_items_active.select(\n",
    "    \"order_id\",\n",
    "    \"order_item_id\",\n",
    "    \"product_id\",\n",
    "    \"seller_id\",\n",
    "    F.col(\"price\").alias(\"item_price\"),\n",
    "    F.col(\"freight_value\").alias(\"item_freight_value\")\n",
    ")\n",
    "\n",
    "print(f\"  ✓ Base: {fact_orders.count():,} rows\")\n",
    "\n",
    "# Join with enriched orders\n",
    "print(\"\\nJoining with orders...\")\n",
    "\n",
    "fact_orders = fact_orders.join(\n",
    "    orders_enriched.select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"order_purchase_timestamp\",\n",
    "        \"order_delivered_customer_date\",\n",
    "        \"order_estimated_delivery_date\",\n",
    "        \"order_status\",\n",
    "        \"delivery_days\",\n",
    "        \"delivery_delay_flag\"\n",
    "    ),\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"  ✓ Orders joined: {fact_orders.count():,} rows\")\n",
    "\n",
    "# Join with aggregated payments\n",
    "print(\"\\nJoining with payments...\")\n",
    "\n",
    "fact_orders = fact_orders.join(\n",
    "    payments_agg,\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"  ✓ Payments joined: {fact_orders.count():,} rows\")\n",
    "\n",
    "# Join with aggregated reviews\n",
    "print(\"\\nJoining with reviews...\")\n",
    "\n",
    "fact_orders = fact_orders.join(\n",
    "    reviews_agg,\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"  ✓ Reviews joined: {fact_orders.count():,} rows\")\n",
    "\n",
    "# Create date dimension keys\n",
    "print(\"\\nCreating date dimension keys...\")\n",
    "\n",
    "fact_orders = fact_orders.withColumn(\n",
    "    \"order_purchase_date_id\",\n",
    "    F.date_format(F.col(\"order_purchase_timestamp\"), \"yyyyMMdd\").cast(\"int\")\n",
    ").withColumn(\n",
    "    \"order_delivery_date_id\",\n",
    "    F.when(\n",
    "        F.col(\"order_delivered_customer_date\").isNotNull(),\n",
    "        F.date_format(F.col(\"order_delivered_customer_date\"), \"yyyyMMdd\").cast(\"int\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Create calculated measures\n",
    "print(\"\\nCreating calculated measures...\")\n",
    "\n",
    "fact_orders = fact_orders.withColumn(\n",
    "    \"total_item_amount\",\n",
    "    F.col(\"item_price\") + F.col(\"item_freight_value\")\n",
    ").withColumn(\n",
    "    \"is_delivered\",\n",
    "    F.when(F.col(\"order_delivered_customer_date\").isNotNull(), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"is_on_time\",\n",
    "    F.when(F.col(\"delivery_delay_flag\") == \"on_time\", 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"profit\",\n",
    "    F.when(\n",
    "        F.col(\"order_delivered_customer_date\").isNotNull(),\n",
    "        F.col(\"total_payment_value\") - F.col(\"item_price\") - F.col(\"item_freight_value\")\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Select final columns\n",
    "fact_orders = fact_orders.select(\n",
    "    # Keys\n",
    "    \"order_id\",\n",
    "    \"order_item_id\",\n",
    "    # Dimension Foreign Keys\n",
    "    \"customer_id\",\n",
    "    \"seller_id\",\n",
    "    \"product_id\",\n",
    "    \"order_purchase_date_id\",\n",
    "    \"order_delivery_date_id\",\n",
    "    # Order Details\n",
    "    \"order_status\",\n",
    "    \"delivery_delay_flag\",\n",
    "    # Measures\n",
    "    \"item_price\",\n",
    "    \"item_freight_value\",\n",
    "    \"total_item_amount\",\n",
    "    \"total_payment_value\",\n",
    "    \"payment_count\",\n",
    "    \"max_installments\",\n",
    "    \"avg_review_score\",\n",
    "    \"total_reviews\",\n",
    "    \"delivery_days\",\n",
    "    \"is_delivered\",\n",
    "    \"is_on_time\",\n",
    "    \"profit\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ FACT_ORDERS created: {fact_orders.count():,} rows\")\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nFACT_ORDERS Statistics:\")\n",
    "fact_orders.select(\n",
    "    F.count(\"*\").alias(\"total_rows\"),\n",
    "    F.countDistinct(\"order_id\").alias(\"unique_orders\"),\n",
    "    F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    F.countDistinct(\"product_id\").alias(\"unique_products\"),\n",
    "    F.countDistinct(\"seller_id\").alias(\"unique_sellers\"),\n",
    "    F.sum(\"is_delivered\").alias(\"delivered_orders\"),\n",
    "    F.sum(\"is_on_time\").alias(\"on_time_deliveries\"),\n",
    "    F.round(F.sum(\"total_payment_value\"), 2).alias(\"total_revenue\"),\n",
    "    F.round(F.sum(\"profit\"), 2).alias(\"total_profit\")\n",
    ").show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c939a6cc-7e11-4e2d-af46-4d150acfd71a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SECTION : Star Schema Summary\n",
    "Display final row counts and schema information for all star schema tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f1b01f-298b-49c6-833e-c5f7155ba302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA STAR SCHEMA TABLES CREATED:\n\nDIMENSION TABLES:\n  ✓ DIM_CUSTOMERS:                              99,441 rows\n  ✓ DIM_SELLERS:                                 3,095 rows\n  ✓ DIM_PRODUCTS:                               32,951 rows\n  ✓ DIM_DATE:                                      799 rows\n\nFACT TABLE:\n  ✓ FACT_ORDERS:                               112,650 rows\n\nTOTAL ROWS ACROSS ALL TABLES:                  248,936\n\n====================================================================================================\n✓ Star schema transformation completed successfully!\n====================================================================================================\n\nThese tables are ready to be exported to Snowflake:\n  1. DIM_CUSTOMERS (deduplicated, with geolocation)\n  2. DIM_SELLERS (deduplicated, with geolocation)\n  3. DIM_PRODUCTS (deduplicated, with Spanish & English categories)\n  4. DIM_DATE (generated from order dates)\n  5. FACT_ORDERS (all measures and dimension keys)\n\nAll tables are clean and ready for analytics!\n\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\uD83D\uDCCA STAR SCHEMA TABLES CREATED:\\n\")\n",
    "\n",
    "# Dimension Tables\n",
    "print(\"DIMENSION TABLES:\")\n",
    "print(f\"  ✓ DIM_CUSTOMERS:                        {dim_customers.count():>12,} rows\")\n",
    "print(f\"  ✓ DIM_SELLERS:                          {dim_sellers.count():>12,} rows\")\n",
    "print(f\"  ✓ DIM_PRODUCTS:                         {dim_products.count():>12,} rows\")\n",
    "print(f\"  ✓ DIM_DATE:                             {dim_date.count():>12,} rows\")\n",
    "\n",
    "# Fact Table\n",
    "print(\"\\nFACT TABLE:\")\n",
    "print(f\"  ✓ FACT_ORDERS:                          {fact_orders.count():>12,} rows\")\n",
    "\n",
    "# Total\n",
    "total_rows = (\n",
    "    dim_customers.count() + \n",
    "    dim_sellers.count() + \n",
    "    dim_products.count() + \n",
    "    dim_date.count() + \n",
    "    fact_orders.count()\n",
    ")\n",
    "print(f\"\\nTOTAL ROWS ACROSS ALL TABLES:             {total_rows:>12,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"✓ Star schema transformation completed successfully!\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\"\"\n",
    "These tables are ready to be exported to Snowflake:\n",
    "  1. DIM_CUSTOMERS (deduplicated, with geolocation)\n",
    "  2. DIM_SELLERS (deduplicated, with geolocation)\n",
    "  3. DIM_PRODUCTS (deduplicated, with Spanish & English categories)\n",
    "  4. DIM_DATE (generated from order dates)\n",
    "  5. FACT_ORDERS (all measures and dimension keys)\n",
    "\n",
    "All tables are clean and ready for analytics!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6887c69-1ddb-4e47-9042-cd81761f4b96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SECTION : Export Star Schema Tables to Snowflake\n",
    "Write all star schema tables to Snowflake using Apache Spark connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a61295-7e45-4a6f-8389-d5cc54c21ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nConfiguring Snowflake connection...\n\n✓ Snowflake configuration loaded\n  Database: ECOMMERCE_DB\n  Schema: my_schema\n  Warehouse: ECOMMERCE_DWH\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Snowflake Connection Configuration\n",
    "print(\"\\nConfiguring Snowflake connection...\\n\")\n",
    "\n",
    "SNOWFLAKE_CONFIG = {\n",
    "    \"sfUrl\": os.getenv(\"sfURL\"),\n",
    "    \"sfUser\": os.getenv(\"sfUser\"),\n",
    "    \"sfPassword\": os.getenv(\"sfPassword\"),\n",
    "    \"sfDatabase\": \"ECOMMERCE_DB\",\n",
    "    \"sfSchema\": \"my_schema\",\n",
    "    \"sfWarehouse\": \"ECOMMERCE_DWH\",\n",
    "    \"sfRole\": \"ACCOUNTADMIN\"\n",
    "}\n",
    "\n",
    "print(\"✓ Snowflake configuration loaded\")\n",
    "print(f\"  Database: {SNOWFLAKE_CONFIG['sfDatabase']}\")\n",
    "print(f\"  Schema: {SNOWFLAKE_CONFIG['sfSchema']}\")\n",
    "print(f\"  Warehouse: {SNOWFLAKE_CONFIG['sfWarehouse']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb557841-ade3-4781-b62b-181f84a1b7fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write Dimension Tables\n",
    "Export all dimension tables (DIM_CUSTOMERS, DIM_SELLERS, DIM_PRODUCTS, DIM_DATE) to Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5b7b9cc-5b7d-4158-ad47-88426a183e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n====================================================================================================\nWRITING DIMENSION TABLES TO SNOWFLAKE\n====================================================================================================\n\nDimension Tables:\n  Writing DIM_CUSTOMERS... ✓ Success (99,441 rows)\n  Writing DIM_SELLERS... ✓ Success (3,095 rows)\n  Writing DIM_PRODUCTS... ✓ Success (32,951 rows)\n  Writing DIM_DATE... ✓ Success (799 rows)\n\n✓ All dimension tables written to Snowflake\n"
     ]
    }
   ],
   "source": [
    "def write_to_snowflake(df, table_name, mode=\"overwrite\"):\n",
    "    \"\"\"\n",
    "    Write DataFrame to Snowflake table\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to write\n",
    "        table_name: Snowflake table name \n",
    "        mode: write mode - \"overwrite\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"  Writing {table_name}...\", end=\" \")\n",
    "        \n",
    "        df.write \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**SNOWFLAKE_CONFIG) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"truncateTable\", \"true\") \\\n",
    "            .mode(mode) \\\n",
    "            .save()\n",
    "        \n",
    "        row_count = df.count()\n",
    "        print(f\"✓ Success ({row_count:,} rows)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed\")\n",
    "        print(f\"    Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"WRITING DIMENSION TABLES TO SNOWFLAKE\")\n",
    "print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "# Write dimension tables\n",
    "results = {}\n",
    "\n",
    "print(\"Dimension Tables:\")\n",
    "results[\"DIM_CUSTOMERS\"] = write_to_snowflake(dim_customers, \"DIM_CUSTOMERS\", mode=\"overwrite\")\n",
    "results[\"DIM_SELLERS\"] = write_to_snowflake(dim_sellers, \"DIM_SELLERS\", mode=\"overwrite\")\n",
    "results[\"DIM_PRODUCTS\"] = write_to_snowflake(dim_products, \"DIM_PRODUCTS\", mode=\"overwrite\")\n",
    "results[\"DIM_DATE\"] = write_to_snowflake(dim_date, \"DIM_DATE\", mode=\"overwrite\")\n",
    "\n",
    "print(\"\\n✓ All dimension tables written to Snowflake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8740fcdc-eae1-48d9-8783-a574d7e95e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write Fact Table\n",
    "Export the central FACT_ORDERS table (contains all measures and dimension foreign keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a152955e-1d4c-4260-a94c-623dca5a028a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n====================================================================================================\nWRITING FACT TABLE TO SNOWFLAKE\n====================================================================================================\n\nFact Table:\n  Writing FACT_ORDERS... ✓ Success (112,650 rows)\n\n✓ Fact table written to Snowflake\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"WRITING FACT TABLE TO SNOWFLAKE\")\n",
    "print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "# Write fact table\n",
    "print(\"Fact Table:\")\n",
    "results[\"FACT_ORDERS\"] = write_to_snowflake(fact_orders, \"FACT_ORDERS\", mode=\"overwrite\")\n",
    "\n",
    "print(\"\\n✓ Fact table written to Snowflake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f89cd4c1-fab4-4f82-80b1-f8f6ed94cae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Export Summary & Validation\n",
    "Verify all tables were successfully exported to Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d41fdea-4eb8-4ccf-839e-44a28602163d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n====================================================================================================\nSNOWFLAKE EXPORT SUMMARY\n====================================================================================================\n\nExport Results:\n  ✓ SUCCESS: DIM_CUSTOMERS\n  ✓ SUCCESS: DIM_SELLERS\n  ✓ SUCCESS: DIM_PRODUCTS\n  ✓ SUCCESS: DIM_DATE\n  ✓ SUCCESS: FACT_ORDERS\n\nTotal: 5/5 tables exported successfully\n\n✅ ALL STAR SCHEMA TABLES SUCCESSFULLY EXPORTED TO SNOWFLAKE!\n\nSnowflake Location:\n  Database: ECOMMERCE_DB\n  Schema: my_schema\n\nTables created:\n  • DIM_CUSTOMERS (99,441 rows)\n  • DIM_SELLERS (3,095 rows)\n  • DIM_PRODUCTS (32,951 rows)\n  • DIM_DATE (799 rows)\n  • FACT_ORDERS (112,650 rows)\n\n====================================================================================================\nExport Complete\n====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"SNOWFLAKE EXPORT SUMMARY\")\n",
    "print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "# Summary\n",
    "success_count = sum(1 for v in results.values() if v)\n",
    "total_count = len(results)\n",
    "\n",
    "print(\"Export Results:\")\n",
    "for table_name, success in results.items():\n",
    "    status = \"✓ SUCCESS\" if success else \"✗ FAILED\"\n",
    "    print(f\"  {status}: {table_name}\")\n",
    "\n",
    "print(f\"\\nTotal: {success_count}/{total_count} tables exported successfully\\n\")\n",
    "\n",
    "if success_count == total_count:\n",
    "    print(\"✅ ALL STAR SCHEMA TABLES SUCCESSFULLY EXPORTED TO SNOWFLAKE!\")\n",
    "    print(f\"\\nSnowflake Location:\")\n",
    "    print(f\"  Database: {SNOWFLAKE_CONFIG['sfDatabase']}\")\n",
    "    print(f\"  Schema: {SNOWFLAKE_CONFIG['sfSchema']}\")\n",
    "    print(f\"\\nTables created:\")\n",
    "    print(f\"  • DIM_CUSTOMERS ({dim_customers.count():,} rows)\")\n",
    "    print(f\"  • DIM_SELLERS ({dim_sellers.count():,} rows)\")\n",
    "    print(f\"  • DIM_PRODUCTS ({dim_products.count():,} rows)\")\n",
    "    print(f\"  • DIM_DATE ({dim_date.count():,} rows)\")\n",
    "    print(f\"  • FACT_ORDERS ({fact_orders.count():,} rows)\")\n",
    "else:\n",
    "    print(\"⚠️  SOME TABLES FAILED TO EXPORT\")\n",
    "    print(\"Please check the error messages above and verify your Snowflake credentials.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"Export Complete\")\n",
    "print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "star_schema_transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}